# DataSuperAgent - Smart Data Analysis Assistant

DataSuperAgent is a powerful and intuitive Streamlit web application designed for interactive data analysis. It empowers users to upload their datasets (CSV or Excel files), combine them seamlessly, and then engage in a conversation with an AI-powered agent to explore, analyze, and visualize the data.

## What is DataSuperAgent?

DataSuperAgent is a Streamlit-based web application built to simplify the data analysis workflow. It allows users to directly interact with their data through a conversational AI, making complex data queries and visualizations accessible without needing to write code manually.

Key capabilities of the DataSuperAgent include:

*   **File Upload:** Easily upload data in popular formats like CSV (Comma Separated Values) and Excel (XLS, XLSX).
*   **Data Combination:** If you have multiple files with similar structures, the agent can combine them into a single dataset for comprehensive analysis.
*   **AI-Powered Analysis:** At its core, the agent utilizes Google's Gemini language model, accessed via the Langchain framework. This allows you to ask questions about your data in natural language.
*   **Code Execution:** The agent translates your natural language queries into Python code (primarily using the Pandas library) to perform actual data manipulations, calculations, filtering, aggregations, and statistical analysis.
*   **Plot Generation:** Request various plots and charts (e.g., line charts, bar charts, histograms). The agent generates these using Matplotlib and displays them directly in the chat interface.
*   **Follow-up Suggestions:** To facilitate deeper exploration, the agent provides relevant follow-up questions you might want to ask based on the current analysis.

## Features

DataSuperAgent comes packed with features to streamline your data analysis process:

*   **Intuitive Web Interface:** Built with Streamlit for a user-friendly experience.
*   **Multi-File Upload:** Supports uploading multiple CSV and Excel (`.xls`, `.xlsx`) files.
*   **Automatic Data Combination:** Intelligently merges uploaded files into a unified Pandas DataFrame.
*   **AI-Powered Conversational Analysis:** Leverage the power of Google's Gemini LLM through Langchain to query your data using natural language.
*   **Dynamic Python/Pandas Code Generation:** The agent generates and executes Python code (primarily Pandas) to perform data operations based on your requests.
*   **Interactive Plot Generation:** Visualize your data with various plots (bar charts, line graphs, histograms, etc.) generated by Matplotlib and displayed in the app.
*   **Contextual Follow-up Suggestions:** Get intelligent suggestions for further questions to explore your data more deeply.
*   **Clear Error Handling:** Provides feedback on errors during file processing or agent execution.
*   **Efficient Caching:** Utilizes Streamlit's caching mechanisms for loaded data and AI model instances to improve performance on subsequent runs.
*   **Chat History:** Keeps track of your conversation with the agent.
*   **Easy Setup & Execution:** Simple to get running with standard Python dependencies.

## Technical Specifications

The DataSuperAgent is built using a modern stack of Python libraries and AI technologies:

*   **Core Language:** Python (3.7+)
*   **Web Framework:** Streamlit - For creating the interactive user interface.
*   **Data Handling:** Pandas - For all data manipulation, analysis, and DataFrame operations.

*   **AI & Machine Learning:**
    *   **Langchain:** A framework for developing applications powered by language models. Used here to create and manage the Pandas DataFrame Agent.
    *   **Google Gemini:** The core Large Language Model (LLM) used for understanding queries and generating responses/code. The script specifically uses `gemini-2.0-flash` for both the main analysis agent and for generating follow-up suggestions.
    *   **Langchain Google Generative AI integration (`langchain-google-genai`):** Provides the interface to Google's generative AI models.

*   **Plotting:**
    *   **Matplotlib:** Used for generating static plots. The `'Agg'` backend is employed for non-interactive server-side plot generation.

*   **File Parsing:**
    *   **Openpyxl:** A Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files. Used for loading `.xlsx` and `.xls` files.

*   **Key Python Libraries (as per `requirements.txt` and script usage):**
    *   `streamlit`
    *   `pandas`
    *   `langchain`
    *   `langchain-experimental` (specifically for `create_pandas_dataframe_agent`)
    *   `langchain-google-genai`
    *   `google-api-python-client` (often a dependency for Google Cloud service interactions)
    *   `matplotlib`
    *   `openpyxl`
    *   `tabulate` (Though not directly in `super_agent-v2.py`, it's a common Langchain dependency for table formatting).

## Process Flow (How it Works)

The DataSuperAgent operates through a series of interconnected steps, combining user interactions with backend processing:

1.  **Application Startup & Initialization:**
    *   When you run `streamlit run super_agent-v2.py`, the Streamlit application starts.
    *   **API Key Check:** The script first verifies if the `GOOGLE_API_KEY` is set (either in the script directly or as an environment variable). If not found, it displays an error and halts.
    *   **LLM Initialization:** Two instances of the Google Gemini LLM (`gemini-2.0-flash`) are initialized using `ChatGoogleGenerativeAI` from Langchain: one for the main data analysis agent and another for generating follow-up question suggestions. These are cached using `@st.cache_resource` for efficiency.
    *   **Plot Directory:** A directory named `temp_plots` is created (if it doesn't already exist) to store temporary plot images.

2.  **Step 1: Data Input (User Interface):**
    *   The user is presented with a file uploader ("Step 1: Upload Data Files").
    *   They can select one or more CSV or Excel (`.xls`, `.xlsx`) files from their local system.

3.  **Step 2: Data Loading & Combination (User Action & Backend Processing):**
    *   The user clicks the "Load and Combine Uploaded Files" button ("Step 2: Load and Combine Data").
    *   The `load_and_combine_files` function (cached with `@st.cache_data`) is triggered:
        *   It iterates through each uploaded file.
        *   CSV files are read using `pd.read_csv()`.
        *   Excel files are read using `pd.read_excel(engine='openpyxl')`.
        *   A summary of the loaded files (name, type, rows) is displayed.
        *   If multiple DataFrames are created, `pd.concat()` attempts to combine them into a single DataFrame. If files have vastly different structures and concatenation fails, a warning is shown, and it may proceed with the first successfully loaded DataFrame.
        *   The resulting combined DataFrame is stored in Streamlit's session state (`st.session_state.combined_df`).
        *   A preview (the first 5 rows) of the combined data is displayed in the UI.

4.  **Step 3: AI Agent Preparation (User Action & Backend Processing):**
    *   The user clicks the "Prepare Agent" button ("Step 3: Prepare AI Agent for Analysis").
    *   The `create_agent` function (cached with `@st.cache_resource`) is called:
        *   It takes the combined DataFrame and the pre-initialized agent LLM as input.
        *   It defines a detailed `agent_instructions_prefix`. This prefix is crucial as it guides the LLM on how to behave, how to use Pandas for analysis, when to generate code, how to format results (especially tables), and importantly, how to handle plotting (specifically, to save plots to `temp_plots/temp_plot.png` and use `plt.close()`).
        *   The Langchain `create_pandas_dataframe_agent` is used to construct the agent. This agent is specifically designed to interact with Pandas DataFrames.
        *   The initialized agent is stored in `st.session_state.agent`, and a flag `st.session_state.agent_ready` is set to `True`.
        *   A success message is displayed in the UI.

5.  **Step 4: Interactive Analysis & Chat (User Interaction & Backend Processing):**
    *   The user types a question or instruction into the chat input field ("Step 4: Chat with your Data Agent") and hits Enter.
    *   The user's message is added to the chat history.
    *   **Agent Execution:**
        *   If the agent is marked as ready:
            *   Any existing `temp_plot.png` from a previous turn is deleted to ensure a fresh plot.
            *   The agent's `.run()` (or `.invoke()`) method is called with the user's prompt.
            *   **LLM Thought Process (Langchain Agent):** The agent (powered by the Gemini LLM) interprets the user's query. Based on its instructions and the query, it determines if it needs to execute Python/Pandas code.
                *   If code is needed, the LLM formulates the appropriate Pandas commands.
                *   This code is then executed by the agent against the `df` (the combined DataFrame).
            *   **Plotting:** If the user asked for a plot, the generated code will include Matplotlib commands. The agent's instructions ensure it saves the plot to `temp_plots/temp_plot.png` and then calls `plt.close()` to free up resources.
            *   **Response Generation:** The agent formulates a textual answer based on the results of the executed code (or directly if no code execution was needed for a general query).
            *   The system checks if `temp_plots/temp_plot.png` exists.
        *   **Displaying Results:**
            *   The agent's textual response is displayed in the chat.
            *   If `temp_plots/temp_plot.png` was created during this turn, the image is also displayed in the chat below the text.
        *   **Follow-up Suggestions:**
            *   The `get_followup_suggestions` function is called. It sends the original prompt, the agent's answer, the DataFrame's column names, and the suggestion LLM to generate 3 relevant follow-up questions.
            *   These suggestions are displayed as clickable buttons in the UI.

6.  **Iterative Interaction & Refinement:**
    *   The user can continue asking new questions.
    *   Alternatively, they can click one of the suggested follow-up question buttons. If a suggestion is clicked, that suggestion becomes the new input prompt, and the chat process (Step 5) repeats.
    *   The chat history (`st.session_state.messages`) is updated with each user query and agent response.

7.  **Sidebar Utilities:**
    *   **Clear Chat History:** Allows the user to reset the conversation and clear any displayed plots.
    *   **Status & Debug Info:** Provides quick insights into the agent's readiness and basic information about the loaded DataFrame.

This flow enables a dynamic and interactive data analysis experience, where the user guides the exploration through natural language.

## Getting Started (Beginner Friendly Guide)

This guide will walk you through setting up and running the DataSuperAgent on your local machine.

### Prerequisites

Before you begin, ensure you have the following:

1.  **Python:** You'll need Python installed on your system. Version 3.7 or higher is recommended. You can download it from [python.org](https://www.python.org/downloads/).
2.  **Google AI Studio Account & API Key:** The agent uses Google's Gemini Pro LLM.
    *   Go to [Google AI Studio](https://aistudio.google.com/).
    *   Sign in with your Google account.
    *   Create a new API key. (Look for a "Get API key" option or similar in your project settings).
    *   **Important:** Copy this API key and keep it safe. You'll need it in the next steps.

### Setup Instructions

1.  **Download the Files:**
    *   Download the following files into a new directory on your computer:
        *   `super_agent-v2.py`
        *   `requirements.txt`

2.  **Configure the Google API Key:**
    You need to tell the script what your API key is. There are two ways to do this:

    *   **Option A: Directly Edit the Script (Simpler for beginners)**
        1.  Open the `super_agent-v2.py` file in a text editor (like Notepad, VS Code, Sublime Text, etc.).
        2.  Find this line (usually near the top):
            ```python
            os.environ["GOOGLE_API_KEY"] = "AIzaSyDN9tpQVBIPlm8gP3IdI3OWLY2eqv5EiDY"  # Replace with your actual API key
            ```
        3.  **Replace `"AIzaSyDN9tpQVBIPlm8gP3IdI3OWLY2eqv5EiDY"` with your actual Google API key that you obtained from AI Studio.** Make sure your key is within the quotation marks.
        4.  Save the file.

    *   **Option B: Set an Environment Variable (More secure, recommended for advanced users)**
        Instead of editing the script, you can set an environment variable for `GOOGLE_API_KEY`. The script will automatically pick it up.
        *   **On macOS or Linux:**
            Open your terminal and type:
            ```bash
            export GOOGLE_API_KEY='YOUR_ACTUAL_API_KEY'
            ```
            Replace `YOUR_ACTUAL_API_KEY` with your real key. To make this permanent, you might need to add this line to your shell's profile file (e.g., `.bashrc`, `.zshrc`).
        *   **On Windows:**
            Open Command Prompt or PowerShell and type:
            *   Command Prompt:
                ```bash
                set GOOGLE_API_KEY=YOUR_ACTUAL_API_KEY
                ```
            *   PowerShell:
                ```powershell
                $env:GOOGLE_API_KEY='YOUR_ACTUAL_API_KEY'
                ```
            Replace `YOUR_ACTUAL_API_KEY` with your real key. To make this permanent, you might need to set it through the System Properties > Environment Variables dialog.

3.  **Install Dependencies:**
    1.  **Open your terminal or command prompt.**
    2.  **Navigate to the directory** where you saved `super_agent-v2.py` and `requirements.txt`. For example, if you saved them in a folder named `DataSuperAgent` on your Desktop, you might type:
        ```bash
        cd Desktop/DataSuperAgent
        ```
    3.  **(Recommended) Create and activate a virtual environment:** This keeps the project's dependencies isolated.
        ```bash
        python -m venv venv
        ```
        Then activate it:
        *   On macOS or Linux: `source venv/bin/activate`
        *   On Windows: `venv\Scripts\activate`
        You should see `(venv)` at the beginning of your terminal prompt.
    4.  **Install the required Python libraries:**
        ```bash
        pip install -r requirements.txt
        ```
        This command reads the `requirements.txt` file and installs all the necessary libraries.

4.  **Run the Application:**
    1.  Ensure your virtual environment is still active (if you created one).
    2.  In your terminal or command prompt, while in the directory containing `super_agent-v2.py`, run the following command:
        ```bash
        python -m streamlit run super_agent.py
        ```
    3.  This will start the Streamlit application. It should automatically open in your default web browser (usually at an address like `http://localhost:8501`). If it doesn't, your terminal will display the local URL you can open manually.

5.  **Using the DataSuperAgent:**
    Once the application is open in your browser, follow these steps on the web page:
    *   **Step 1: Upload Data Files:** Click "Browse files" to select your CSV or Excel files.
    *   **Step 2: Load and Combine Data:** After uploading, click the "Load and Combine Uploaded Files" button. You'll see a summary and a preview of your data.
    *   **Step 3: Prepare AI Agent:** Click the "Prepare Agent" button. Wait for the success message indicating the AI agent is ready.
    *   **Step 4: Chat with your Data Agent:** Now you can type your questions about the data into the chat input at the bottom of the page and press Enter. The agent will respond, and if you ask for plots, they will appear in the chat.

You're all set! Start exploring your data with the DataSuperAgent.

## Customization (Optional)

While DataSuperAgent works out-of-the-box, you can customize certain aspects if you have specific needs:

1.  **Changing LLM Models:**
    *   The script uses `gemini-2.0-flash` for both the main agent and suggestions by default. You can change this by modifying the `AGENT_MODEL_NAME` and `SUGGESTION_MODEL_NAME` variables near the top of the `super_agent-v2.py` script.
        ```python
        AGENT_MODEL_NAME = "gemini-2.0-flash"
        SUGGESTION_MODEL_NAME = "gemini-2.0-flash" # Or choose a different one if desired
        ```
    *   Ensure the model you choose is compatible with the `ChatGoogleGenerativeAI` interface and your API key permissions.

2.  **Modifying Agent Behavior (Advanced):**
    *   The agent's core behavior, especially how it responds to queries and uses tools, is heavily influenced by the `agent_instructions_prefix` within the `create_agent` function.
    *   Advanced users can carefully modify this prompt to change the agent's persona, response style, or how it handles specific tasks. However, this requires a good understanding of prompting LLMs and the Langchain agent framework. Be cautious when making changes here, as it can significantly impact the agent's performance and ability to use tools correctly (like plotting).

3.  **Extending File Type Support:**
    *   Currently, the agent supports CSV and Excel files. To add support for other file types (e.g., JSON, Parquet), you would need to:
        *   Modify the `st.file_uploader` in the UI to accept the new file extensions.
        *   Update the `load_and_combine_files` function to include logic for reading the new file type into a Pandas DataFrame (e.g., using `pd.read_json()` or `pd.read_parquet()`).

4.  **Adjusting Temperature for LLMs:**
    *   The `get_llm` function initializes the LLMs with a specific `temperature`.
        *   `llm_agent_model = get_llm(model_name=AGENT_MODEL_NAME, temperature=0)`
        *   `llm_suggestion_model = get_llm(model_name=SUGGESTION_MODEL_NAME, temperature=0.7)`
    *   A temperature of `0` (for the main agent) makes the output more deterministic and focused. A higher temperature like `0.7` (for suggestions) allows for more creativity. You can experiment with these values if you desire different response characteristics.

Remember to test thoroughly after making any customizations.

## Troubleshooting

Here are some common issues you might encounter and how to resolve them:

1.  **Error: `GOOGLE_API_KEY environment variable not set!` or Authentication Issues:**
    *   **Cause:** The script cannot find your Google API key.
    *   **Solution:**
        *   Ensure you have correctly set the `GOOGLE_API_KEY` either directly in the `super_agent-v2.py` script (by replacing the placeholder) or as an environment variable (and that the environment variable was set in the same terminal session where you are running `streamlit run ...`).
        *   Double-check that the API key itself is correct and has been copied accurately from Google AI Studio.
        *   Ensure your Google Cloud project associated with the API key has the "Generative Language API" (or similar, the name might vary) enabled.
        *   If using an environment variable, you might need to restart your terminal or even your computer for the changes to take full effect in some cases.

2.  **Error: `ModuleNotFoundError: No module named 'pandas'` (or similar for other libraries):**
    *   **Cause:** Required Python libraries are not installed.
    *   **Solution:**
        *   Make sure you have run `pip install -r requirements.txt` in your project's directory (and within your activated virtual environment, if you're using one).
        *   If you added new dependencies or changed `requirements.txt`, run the command again.

3.  **Application Fails to Start or Behaves Unexpectedly:**
    *   **Cause:** Could be various issues, often related to recent changes or environment problems.
    *   **Solution:**
        *   Check the terminal where you ran `streamlit run super_agent-v2.py` for any error messages. These messages are often very helpful in diagnosing the problem.
        *   If you recently made changes to the script, try reverting them to see if the issue resolves.
        *   Ensure all file paths (like `TEMP_PLOT_DIR`) are correctly specified and that the script has permissions to create directories/files if needed.

4.  **Plots Not Appearing or "Plot image not found":**
    *   **Cause:** The agent might have failed to save the plot, or the path is incorrect.
    *   **Solution:**
        *   Check the terminal output when you ask for a plot. The agent's `verbose=True` setting (if active during development, though it's set to `True` in the provided `create_agent` function) prints its thoughts and actions, which can reveal if it attempted to create and save a plot.
        *   Ensure the `TEMP_PLOT_DIR` (`temp_plots`) exists in the same directory as `super_agent-v2.py` and that the script has write permissions there. The script attempts to create it with `os.makedirs(TEMP_PLOT_DIR, exist_ok=True)`, but permissions issues could still arise.
        *   The agent is specifically instructed to save to `TEMP_PLOT_FILE` (which is `temp_plots/temp_plot.png`). If this instruction is accidentally altered in the agent's prefix, it might save elsewhere or not at all.

5.  **Slow Responses from the Agent:**
    *   **Cause:** LLM processing can take time, especially for complex queries or if the API is experiencing high load.
    *   **Solution:**
        *   Be patient, especially for the first query after loading data or preparing the agent, as models and data might be loading into memory.
        *   Ensure you have a stable internet connection.
        *   For very large datasets, the agent might be slower as more data context might be implicitly passed or processed.

6.  **`openpyxl` related error when loading Excel files (e.g., `ImportError: Missing optional dependency 'openpyxl'`):**
    *   **Cause:** `openpyxl` is needed for `.xlsx` files but might not have been installed correctly.
    *   **Solution:**
        *   Ensure `openpyxl` is listed in your `requirements.txt` and run `pip install -r requirements.txt` again.
        *   You can also try `pip install openpyxl` directly.

If you encounter issues not listed here, the error messages in your terminal are the best starting point for diagnosis.
